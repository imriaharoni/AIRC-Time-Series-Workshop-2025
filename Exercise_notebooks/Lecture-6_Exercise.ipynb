{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Deep Learning for Time Series Analysis - Exercise\n",
    "### **Learning Objectives**:\n",
    "- Apply feature engineering techniques specifically for deep learning models.\n",
    "- Building and Training RNN based Models (Simple RNN, LSTM, GRU)\n",
    "- Implement sequence-to-sequence prediction for multi-step time series forecasting.\n",
    "- Understand the importance of hyperparameters for LSTM and GRU models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "from pydataset import data                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Select Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Beijing_Air_Quality data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='Data_sets/Beijing_Air_Quality.csv'\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "bjn_a_q_data = pd.read_csv(path)#, parse_dates=['time'], index_col='time')\n",
    "\n",
    "# Convert the year, month, day, hour columns to a datetime index\n",
    "bjn_a_q_data['datetime'] = pd.to_datetime(bjn_a_q_data[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "# Set the datetime column as the index\n",
    "bjn_a_q_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Drop the year, month, day, hour and No columns\n",
    "bjn_a_q_data.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True)\n",
    "\n",
    "# set bjn_a_q_data frequency to hourly\n",
    "bjn_a_q_data = bjn_a_q_data.asfreq('H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel missing values\n",
    "### Resample the data to daily mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_q_df = bjn_a_q_data.copy()\n",
    "# Drop the columns with non numeric values\n",
    "air_q_df.drop(columns=['station','wd'], inplace=True)\n",
    "\n",
    "# Count the number of missing values by columns\n",
    "missing_values = air_q_df.isnull().sum()\n",
    "print('missing values by columns:', missing_values)\n",
    "\n",
    "# count number of rows with missing values\n",
    "missing_value_rows = air_q_df.isnull().sum(axis=1)\n",
    "missing_value_rows = missing_value_rows[missing_value_rows > 0].count()\n",
    "print('\\n number of rows with missing values:', missing_value_rows)\n",
    "\n",
    "# Since PM2.5 is the target variable, we will drop the rows with missing values in the PM2.5 column\n",
    "air_q_df.dropna(subset=['PM2.5'], inplace=True)\n",
    "\n",
    "# Count the number of leftover missing values by columns\n",
    "missing_values = air_q_df.isnull().sum()\n",
    "\n",
    "# most of missing values are left in the CO and O3 columns\n",
    "# Fill the missing values in the remaining columns with forward fill\n",
    "air_q_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Resample creat a daily avrege form for the data\n",
    "air_q_df = air_q_df.resample('D').mean()  # 'D' stands for daily\n",
    "air_q_df = air_q_df.round(2) # set to 2 digits after the decimal point\n",
    "air_q_df = air_q_df.asfreq('D') # set frequency to daily\n",
    "air_q_df.dropna(inplace=True)\n",
    "\n",
    "# print(air_q_df.info())\n",
    "air_q_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "air_q_df.plot(subplots=True,figsize=(10, 12))\n",
    "#plt.title('Air Quality Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the time series for Deep Learning application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Predictors \n",
    "- Choose the target variable and exogenoues variables to serve as predictors\n",
    "- Rearrange the DataFrame to include the target variable as the first column and the predictors as the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the PM2.5 column as the target variable and climate data as the predictor variables\n",
    "target = ## Your code here\n",
    "\n",
    "# For example- select the climate data columns to use as predictors\n",
    "predictors = ## Your code here\n",
    " \n",
    "# Reset the columns of the DataFrame to the selected columns\n",
    "air_q_df= air_q_df[[target] + predictors] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Create additional meaningful features that help models capture temporal patterns\n",
    "this is optional and may add additional information to the model, for example:\n",
    "- Lagged Features\n",
    "- Rolling Statistics \n",
    "- Seasonal Components\n",
    "\n",
    "##### The model should be tested with or without additional features to evaluate their impact on the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example for possible feature engineering\n",
    "\n",
    "# Adding lag features to capture previous values (choose specific variables and lags)\n",
    "air_q_df['PM2.5_lag1'] = ## Your code here\n",
    "air_q_df['WSPM_1'] = ## Your code here\n",
    "# you can add more lag features\n",
    "\n",
    "# Rolling Mean \n",
    "# Adding rolling mean to capture trend and seasonal components\n",
    "air_q_df['PM2.5_rolling_mean'] = ## Your code here\n",
    "\n",
    "# Adding seasonal components to capture seasonality\n",
    "air_q_df['day_of_week'] = ## Your code here\n",
    "\n",
    "# you can add more statistical features or seasonal components\n",
    "\n",
    "# Drop NaN values that arise from lagging and rolling operations\n",
    "air_q_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Scaling the Data**\n",
    "# choose the scaler to be used, for example MinMaxScaler or StandardScaler\n",
    "scaler = ## Your code here\n",
    "\n",
    " # fit and transform the scaler on the data without the time index\n",
    "scaled_features = ## Your code here\n",
    "\n",
    "# Convert the scaled data into a DataFrame\n",
    "air_q_scaled = pd.DataFrame(scaled_features, columns=[air_q_df.columns])\n",
    "\n",
    "# reset the index to be the tiime index of the original data\n",
    "air_q_scaled.index = air_q_df.index\n",
    "print(air_q_scaled.info())\n",
    "\n",
    "# plot the scaled data\n",
    "air_q_scaled.plot(subplots=True,figsize=(9, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data for Deep Learning Models\n",
    "- Convert feature data into a 3D format suitable for RNN-based models \n",
    "- Required shape: (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_features(df, window_size, variables=None, target_variable='PM2.5'):\n",
    "    \"\"\"\n",
    "    Create lagged features for specified variables based on a given window size.\n",
    "    Prepare time windows for an RNN model.\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrame to collect the lagged values\n",
    "    rnn_df = pd.DataFrame()\n",
    "\n",
    "    # Add the target variable to the rnn_df\n",
    "    rnn_df['target'] = df[target_variable]\n",
    "\n",
    "    # Select the specified predictor variables for creating the time windows\n",
    "    # if no variables are specified, use all columns including the laged values of target variable\n",
    "    if variables is None:\n",
    "        variables = df.columns \n",
    "\n",
    "    df = df[variables] \n",
    "    \n",
    "    # Iterate over the window size in reverse\n",
    "    for lag in range(window_size, 0, -1):\n",
    "        shifted= df.shift(lag)\n",
    "        rnn_df[f'Lag{lag}_values'] = shifted.apply(lambda x: x.values, axis=1)\n",
    "\n",
    "    # Drop the first rows in the df, according to the window size used\n",
    "    rnn_df = rnn_df[window_size:]\n",
    "       \n",
    "    return rnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_RNN=create_window_features(df=air_q_scaled, window_size= '?', variables= '?', target_variable='?')\n",
    "print('df_to_RNN.shape',df_to_RNN.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Splitting X,y Train Test (Using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and target (X, y) for the train, validation, and test data\n",
    "X_set, y_set = df_to_RNN.drop('target', axis=1), df_to_RNN['target']\n",
    "# convert y_set to 2D dataframe\n",
    "y_set = y_set.to_frame()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "# When shuffle=False,  train_test_split will split based on the original order last 15% of the rows will be assigned to test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=0.15, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, shuffle=False)\n",
    "\n",
    "# print the shape of the train, validation, and test sets\n",
    "print('Train set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_val.shape, y_val.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape X data to 3D to fit RNN input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and reshape the training data\n",
    "x_train_flattened = np.array([item for sublist in X_train.values for item in sublist])\n",
    "x_train_RNN_array = x_train_flattened.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
    "\n",
    "# Flatten and reshape the test data\n",
    "x_test_flattened = np.array([item for sublist in X_test.values for item in sublist])\n",
    "x_test_RNN_array = x_test_flattened.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
    "\n",
    "# Flatten and reshape the validation data\n",
    "x_val_flattened = np.array([item for sublist in X_val.values for item in sublist])\n",
    "x_val_RNN_array = x_val_flattened.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
    "\n",
    "# Print the shapes of the reshaped arrays\n",
    "print('x_train_RNN_array.shape:', x_train_RNN_array.shape, \n",
    "      'x_test_RNN_array.shape:', x_test_RNN_array.shape, \n",
    "      'x_val_RNN_array.shape:', x_val_RNN_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    # RNNs Expect Input Data in 3D Shape: (samples=batch size, timesteps, features)\n",
    "    model.add(SimpleRNN(units=units, return_sequences=False, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "    model.add(Dense(1, activation='linear')) # output layer predicting a single numerical value\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN= create_RNN_model(x_train=x_train_RNN_array, units= 32)\n",
    "simple_RNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_RNN.fit(x_train_RNN_array, y_train , epochs=30, batch_size=100, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_RNN.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LSTM= create_LSTM_model(x_train=x_train_RNN_array, units=16)\n",
    "simple_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_LSTM.fit(x_train_RNN_array, y_train , epochs=50, batch_size=50, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_LSTM.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Task: Experimenting with Preprocessing and Model Complexity for RNN Models\n",
    "\n",
    "#### Understand how feature engineering and model parameters affect RNNs performance, and how model complexity impact prediction, overfitting, and generalization\n",
    "\n",
    "#### Feature Engineering for RNN Models:\n",
    "\n",
    "- Select Additional Variables: Choose additional variables from the dataset that may influence the target variable \n",
    "\n",
    "- Create Lagged Features and Rolling Statistics for Selected variables: This helps the model capture temporal dependencies and identify patterns over different time periods\n",
    "\n",
    "- Control the Window Size: Evaluate how larger window sizes impact the model's complexity and potential overfitting\n",
    "\n",
    "#### Deep Learning Model Hyperparameter Tuning:\n",
    "\n",
    "- Number of Layers: Experiment with different numbers of RNN layers (typically between 1 and 3). You may also add Dense layers after the RNN layers to capture non-linear relationships\n",
    "\n",
    "- Number of Units: Modify the number of units in each RNN layer. A higher number increases the model's capacity to learn complex patterns but can also lead to overfitting. Common values range between 16 to 100 units\n",
    "\n",
    "- Dropout Layer: Introduce dropout layers to mitigate overfitting. Test dropout rates between 0.2 to 0.5 to observe their effect on the model's robustness\n",
    "\n",
    "- Early Stopping: Prevent overfitting by controlling the number of training **epochs**. Set a suitable value to determine when training should stop based on validation loss\n",
    "\n",
    "#### Advanced Hyperparameter Tuning (for those with prior knowledg on deep learning models):\n",
    "\n",
    "- Activation Functions: Adjust activation functions in the Dense layers to test their effect on non-linearity capture. Use 'relu' for Dense layers and 'linear' for the output layer\n",
    "\n",
    "- Learning Rate: Experiment with learning rates to control the model's convergence speed\n",
    "\n",
    "- Batch Size: Test different batch sizes to evaluate their effect on training stability and convergence speed. Small batch sizes provide regularization benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex LSTM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    # Add a Dropout layer\n",
    "    #model.add(Dropout(0.2))\n",
    "\n",
    "    # Add a second LSTM layer\n",
    "    #model.add(LSTM(units=?))\n",
    "\n",
    "    # Add a Dense layer\n",
    "    #model.add(Dense(16, activation='relu'))\n",
    "\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LSTM= create_LSTM_model(x_train=x_train_RNN_array, units=50)\n",
    "simple_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_LSTM.fit(x_train_RNN_array, y_train , epochs=50, batch_size=20, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_LSTM.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6 Exercise - Example Solution\n",
    "### **Learning Objectives**:\n",
    "- Apply feature engineering techniques specifically for deep learning models.\n",
    "- Building and Training RNN based Models (Simple RNN, LSTM, GRU)\n",
    "- Implement sequence-to-sequence prediction for multi-step time series forecasting.\n",
    "- Understand the importance of hyperparameters for LSTM and GRU models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "from pydataset import data                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Select Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Beijing_Air_Quality data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='Data_sets/Beijing_Air_Quality.csv'\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "bjn_a_q_data = pd.read_csv(path)#, parse_dates=['time'], index_col='time')\n",
    "\n",
    "# Convert the year, month, day, hour columns to a datetime index\n",
    "bjn_a_q_data['datetime'] = pd.to_datetime(bjn_a_q_data[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "# Set the datetime column as the index\n",
    "bjn_a_q_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Drop the year, month, day, hour and No columns\n",
    "bjn_a_q_data.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True)\n",
    "\n",
    "# set bjn_a_q_data frequency to hourly\n",
    "bjn_a_q_data = bjn_a_q_data.asfreq('H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel missing values\n",
    "### Resample the data to daily mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_q_df = bjn_a_q_data.copy()\n",
    "# Drop the columns with non numeric values\n",
    "air_q_df.drop(columns=['station','wd'], inplace=True)\n",
    "\n",
    "# Count the number of missing values by columns\n",
    "missing_values = air_q_df.isnull().sum()\n",
    "print('missing values by columns:', missing_values)\n",
    "\n",
    "# count number of rows with missing values\n",
    "missing_value_rows = air_q_df.isnull().sum(axis=1)\n",
    "missing_value_rows = missing_value_rows[missing_value_rows > 0].count()\n",
    "print('\\n number of rows with missing values:', missing_value_rows)\n",
    "\n",
    "# Since PM2.5 is the target variable, we will drop the rows with missing values in the PM2.5 column\n",
    "air_q_df.dropna(subset=['PM2.5'], inplace=True)\n",
    "\n",
    "# Count the number of leftover missing values by columns\n",
    "missing_values = air_q_df.isnull().sum()\n",
    "\n",
    "# most of missing values are left in the CO and O3 columns\n",
    "# Fill the missing values in the remaining columns with forward fill\n",
    "air_q_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Resample creat a daily avrege form for the data\n",
    "air_q_df = air_q_df.resample('D').mean()  # 'D' stands for daily\n",
    "air_q_df = air_q_df.round(2) # set to 2 digits after the decimal point\n",
    "air_q_df = air_q_df.asfreq('D') # set frequency to daily\n",
    "air_q_df.dropna(inplace=True)\n",
    "\n",
    "# print(air_q_df.info())\n",
    "air_q_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "air_q_df.plot(subplots=True,figsize=(10, 12))\n",
    "#plt.title('Air Quality Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the time series for Deep Learning application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Predictors \n",
    "- Choose the target variable and exogenoues variables to serve as predictors\n",
    "- Rearrange the DataFrame to include the target variable as the first column and the predictors as the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the PM2.5 column as the target variable and climate data as the predictor variables\n",
    "target = 'PM2.5'\n",
    "# For example- select the climate data columns to use as predictors\n",
    "predictors = ['DEWP', 'TEMP', 'PRES', 'RAIN', 'WSPM'] \n",
    "# Reset the columns of the DataFrame to the selected columns\n",
    "air_q_df= air_q_df[[target] + predictors] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Create additional meaningful features that help models capture temporal patterns\n",
    "this is optional and may add additional information to the model, for example:\n",
    "- Lagged Features\n",
    "- Rolling Statistics \n",
    "- Seasonal Components\n",
    "\n",
    "##### The model should be tested with or without additional features to evaluate their impact on the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example for possible feature engineering\n",
    "\n",
    "# Adding lag features to capture previous values (choose specific variables and lags)\n",
    "air_q_df['PM2.5_lag1'] = air_q_df['PM2.5'].shift(1)\n",
    "air_q_df['WSPM_1'] = air_q_df['WSPM'].shift(1)\n",
    "\n",
    "# Rolling Mean \n",
    "# Adding rolling mean to capture trend and seasonal components\n",
    "air_q_df['PM2.5_rolling_mean'] = air_q_df['PM2.5'].rolling(window=5).mean()\n",
    "\n",
    "# Adding seasonal components to capture seasonality\n",
    "# day of the week\n",
    "air_q_df['day_of_week'] = air_q_df.index.dayofweek\n",
    "\n",
    "# Drop NaN values that arise from lagging and rolling operations\n",
    "air_q_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Scaling the Data**\n",
    "# choose the scaler to be used, for example MinMaxScaler or StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    " # fit and transform the scaler on the data without the time index\n",
    "scaled_features = scaler.fit_transform(air_q_df[air_q_df.columns])\n",
    "\n",
    "# Convert the scaled data into a DataFrame\n",
    "air_q_scaled = pd.DataFrame(scaled_features, columns=[air_q_df.columns])\n",
    "# reset the index to be the tiime index of the original data\n",
    "air_q_scaled.index = air_q_df.index\n",
    "print(air_q_scaled.info())\n",
    "\n",
    "# plot the scaled data\n",
    "air_q_scaled.plot(subplots=True,figsize=(9, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data for Deep Learning Models\n",
    "- Convert feature data into a 3D format suitable for RNN-based models \n",
    "- Required shape: (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_features(df, window_size, variables=None, target_variable='PM2.5'):\n",
    "    \"\"\"\n",
    "    Create lagged features for specified variables based on a given window size.\n",
    "    Prepare time windows for an RNN model.\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrame to collect the lagged values\n",
    "    rnn_df = pd.DataFrame()\n",
    "\n",
    "    # Add the target variable to the rnn_df\n",
    "    rnn_df['target'] = df[target_variable]\n",
    "\n",
    "    # Select the specified predictor variables for creating the time windows\n",
    "    # if no variables are specified, use all columns including the laged values of target variable\n",
    "    if variables is None:\n",
    "        variables = df.columns \n",
    "\n",
    "    df = df[variables] \n",
    "    \n",
    "    # Iterate over the window size in reverse\n",
    "    for lag in range(window_size, 0, -1):\n",
    "        shifted= df.shift(lag)\n",
    "        rnn_df[f'Lag{lag}_values'] = shifted.apply(lambda x: x.values, axis=1)\n",
    "\n",
    "    # Drop the first rows in the df, according to the window size used\n",
    "    rnn_df = rnn_df[window_size:]\n",
    "       \n",
    "    return rnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_RNN=create_window_features(df=air_q_df, window_size=3, variables= None, target_variable='PM2.5')\n",
    "df_to_RNN= df_to_RNN.round(2)\n",
    "df_to_RNN.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_RNN=create_window_features(df=air_q_scaled, window_size=10, variables= predictors, target_variable='PM2.5')\n",
    "print('df_to_RNN.shape',df_to_RNN.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Splitting X,y Train Test (Using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and target (X, y) for the train, validation, and test data\n",
    "X_set, y_set = df_to_RNN.drop('target', axis=1), df_to_RNN['target']\n",
    "# convert y_set to 2D dataframe\n",
    "y_set = y_set.to_frame()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "# When shuffle=False,  train_test_split will split based on the original order last 15% of the rows will be assigned to test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=0.15, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, shuffle=False)\n",
    "\n",
    "# print the shape of the train, validation, and test sets\n",
    "print('Train set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_val.shape, y_val.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape X data to 3D to fit RNN input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and reshape the training data\n",
    "x_train_flattened = np.array([item for sublist in X_train.values for item in sublist])\n",
    "x_train_RNN_array = x_train_flattened.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
    "\n",
    "# Flatten and reshape the test data\n",
    "x_test_flattened = np.array([item for sublist in X_test.values for item in sublist])\n",
    "x_test_RNN_array = x_test_flattened.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
    "\n",
    "# Flatten and reshape the validation data\n",
    "x_val_flattened = np.array([item for sublist in X_val.values for item in sublist])\n",
    "x_val_RNN_array = x_val_flattened.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
    "\n",
    "# Print the shapes of the reshaped arrays\n",
    "print('x_train_RNN_array.shape:', x_train_RNN_array.shape, \n",
    "      'x_test_RNN_array.shape:', x_test_RNN_array.shape, \n",
    "      'x_val_RNN_array.shape:', x_val_RNN_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the first element of the training set\n",
    "x_train_RNN_array[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    # RNNs Expect Input Data in 3D Shape: (samples=batch size, timesteps, features)\n",
    "    model.add(SimpleRNN(units=units, return_sequences=False, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "    model.add(Dense(1, activation='linear')) # output layer predicting a single numerical value\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN= create_RNN_model(x_train=x_train_RNN_array, units= 32)\n",
    "simple_RNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_RNN.fit(x_train_RNN_array, y_train , epochs=50, batch_size=100, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_RNN.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LSTM= create_LSTM_model(x_train=x_train_RNN_array, units=16)\n",
    "simple_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_LSTM.fit(x_train_RNN_array, y_train , epochs=50, batch_size=50, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_LSTM.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GRU_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    # RNNs Expect Input Data in 3D Shape: (samples=batch size, timesteps, features)\n",
    "    model.add(GRU(units=units, return_sequences=False, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "    model.add(Dense(1)) # output layer predicting a single numerical value\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_GRU= create_GRU_model(x_train=x_train_RNN_array, units= 32)\n",
    "simple_GRU.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_GRU.fit(x_train_RNN_array, y_train, epochs=50, batch_size=32, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_GRU.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Task: Experimenting with Preprocessing and Model Complexity for RNN Models\n",
    "\n",
    "#### Understand how feature engineering and model parameters affect RNNs performance, and how model complexity impact prediction, overfitting, and generalization\n",
    "\n",
    "#### Feature Engineering for RNN Models:\n",
    "\n",
    "- Select Additional Variables: Choose additional variables from the dataset that may influence the target variable \n",
    "\n",
    "- Create Lagged Features and Rolling Statistics for Selected variables: This helps the model capture temporal dependencies and identify patterns over different time periods\n",
    "\n",
    "- Control the Window Size: Evaluate how larger window sizes impact the model's complexity and potential overfitting\n",
    "\n",
    "#### Deep Learning Model Hyperparameter Tuning:\n",
    "\n",
    "- Number of Layers: Experiment with different numbers of RNN layers (typically between 1 and 3). You may also add Dense layers after the RNN layers to capture non-linear relationships\n",
    "\n",
    "- Number of Units: Modify the number of units in each RNN layer. A higher number increases the model's capacity to learn complex patterns but can also lead to overfitting. Common values range between 16 to 100 units\n",
    "\n",
    "- Dropout Layer: Introduce dropout layers to mitigate overfitting. Test dropout rates between 0.2 to 0.5 to observe their effect on the model's robustness\n",
    "\n",
    "- Early Stopping: Prevent overfitting by controlling the number of training **epochs**. Set a suitable value to determine when training should stop based on validation loss\n",
    "\n",
    "#### Advanced Hyperparameter Tuning (for those with prior knowledg on deep learning models):\n",
    "\n",
    "- Activation Functions: Adjust activation functions in the Dense layers to test their effect on non-linearity capture. Use 'relu' for Dense layers and 'linear' for the output layer\n",
    "\n",
    "- Learning Rate: Experiment with learning rates to control the model's convergence speed\n",
    "\n",
    "- Batch Size: Test different batch sizes to evaluate their effect on training stability and convergence speed. Small batch sizes provide regularization benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex LSTM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(x_train, units):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    # Add a Dropout layer\n",
    "    #model.add(Dropout(0.2))\n",
    "    # Add a Dense layer\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LSTM= create_LSTM_model(x_train=x_train_RNN_array, units=50)\n",
    "simple_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = simple_LSTM.fit(x_train_RNN_array, y_train , epochs=50, batch_size=20, \n",
    "                            validation_data=(x_val_RNN_array, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss by epoch of train and validation data\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= simple_LSTM.predict(x_test_RNN_array)\n",
    "# inverse transform the scaled y values to the original scale\n",
    "y_pred= pd.Series(y_pred.flatten(), index=y_test.index)\n",
    "\n",
    "# evaluate the model eror \n",
    "rmse = np.sqrt(np.mean((y_test.values - y_pred.values)**2))\n",
    "\n",
    "# Plot the forecast vs actual\n",
    "time_index= y_test.index\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, y_test, label='actual')\n",
    "plt.plot(time_index, y_pred, label='forecast')\n",
    "# print the RMSE on the plot\n",
    "plt.title(f'Forecast vs Actual - RMSE: {rmse:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeSeriesCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
